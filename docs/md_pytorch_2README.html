<!DOCTYPE html PUBLIC "-//W3C//DTD XHTML 1.0 Transitional//EN" "https://www.w3.org/TR/xhtml1/DTD/xhtml1-transitional.dtd">
<html xmlns="http://www.w3.org/1999/xhtml" lang="en-US">
<head>
<meta http-equiv="Content-Type" content="text/xhtml;charset=UTF-8"/>
<meta http-equiv="X-UA-Compatible" content="IE=11"/>
<meta name="generator" content="Doxygen 1.9.8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<title>Lane Detection: Results</title>
<link href="tabs.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="jquery.js"></script>
<script type="text/javascript" src="dynsections.js"></script>
<link href="search/search.css" rel="stylesheet" type="text/css"/>
<script type="text/javascript" src="search/searchdata.js"></script>
<script type="text/javascript" src="search/search.js"></script>
<link href="doxygen.css" rel="stylesheet" type="text/css" />
<link href="doxyfile.css" rel="stylesheet" type="text/css"/>
</head>
<body>
<div id="top"><!-- do not remove this div, it is closed by doxygen! -->
<div id="titlearea">
<table cellspacing="0" cellpadding="0">
 <tbody>
 <tr id="projectrow">
  <td id="projectalign">
   <div id="projectname">Lane Detection
   </div>
  </td>
 </tr>
 </tbody>
</table>
</div>
<!-- end header part -->
<!-- Generated by Doxygen 1.9.8 -->
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
var searchBox = new SearchBox("searchBox", "search/",'.html');
/* @license-end */
</script>
<script type="text/javascript" src="menudata.js"></script>
<script type="text/javascript" src="menu.js"></script>
<script type="text/javascript">
/* @license magnet:?xt=urn:btih:d3d9a9a6595521f9666a5e94cc830dab83b65699&amp;dn=expat.txt MIT */
$(function() {
  initMenu('',true,false,'search.php','Search');
  $(document).ready(function() { init_search(); });
});
/* @license-end */
</script>
<div id="main-nav"></div>
<!-- window showing the filter options -->
<div id="MSearchSelectWindow"
     onmouseover="return searchBox.OnSearchSelectShow()"
     onmouseout="return searchBox.OnSearchSelectHide()"
     onkeydown="return searchBox.OnSearchSelectKey(event)">
</div>

<!-- iframe showing the search results (closed by default) -->
<div id="MSearchResultsWindow">
<div id="MSearchResults">
<div class="SRPage">
<div id="SRIndex">
<div id="SRResults"></div>
<div class="SRStatus" id="Loading">Loading...</div>
<div class="SRStatus" id="Searching">Searching...</div>
<div class="SRStatus" id="NoMatches">No Matches</div>
</div>
</div>
</div>
</div>

</div><!-- top -->
<div><div class="header">
  <div class="headertitle"><div class="title">Results</div></div>
</div><!--header-->
<div class="contents">
<div class="textblock"><p><a class="anchor" id="autotoc_md0"></a> These images were captured during training with TUSimple and CULane datasets, after a certain amount of epochs.</p>
<div class="image">
<img src="tusimple.png" alt=""/>
</div>
    <p>This image is in epoch 37, nightime.</p>
<div class="image">
<img src="epoch37.png" alt=""/>
</div>
    <p>This image was taken whilst testing the model in Jetson. Here you see different masks, with different thresholds of values, after applying the activation function <b>sigmoid</b>. After removing the fisheye effect of Jetson's camera, the model produced much better results.</p>
<div class="image">
<img src="jetson_model.jpeg" alt=""/>
</div>
    <p>Next there are a few videos we recorded while testing the model in CARLA. On one side, you see a CARLA's window, with the vehicle from a top spectator view. On the other side, you see an <b>OpenCV</b> window from the car's perspective, with our pytorch model's binary mask overlaying the road. In intersections, because there is NO lane, we defined that the car should go straight ahead, in the CARLA environment. You can see this behaviour in <em><a class="el" href="carla__setup_8py.html">carla_setup.py</a></em></p>
<p>Click to see <a href="/pytorch/results/town5.mp4">Town5 Demo</a>, and <a href="/pytorch/results/town4.mp4">Town4 Demo</a>.</p>
<h1><a class="anchor" id="autotoc_md1"></a>
Chosen datasets</h1>
<p>The datasets we are using to train the model are from: <a href="https://onedrive.live.com/?id=4EF9629CA3CB4B5E%213022&amp;cid=4EF9629CA3CB4B5E&amp;redeem=aHR0cHM6Ly8xZHJ2Lm1zL3UvcyFBbDVMeTZPY1l2bE9sMDQxNHNSb3BGVkgyOTVXP2U9Q2pjbDYy">This is an external link to the Datasets used</a>. We used dataset8 and dataset10. However, we were using TUSimple and CULane beforehand, but couldn't get good results on CARLA, so we decided to switch datasets. For testing, we used dataset11 and <b>CARLA</b>. We have a testing script for CARLA in <em><a class="el" href="carla__setup_8py.html">carla_setup.py</a></em>.</p>
<h1><a class="anchor" id="autotoc_md2"></a>
Pytorch Model</h1>
<p>We use <b>OpenCV</b> to pre and post-process our images, sending them to our Pytorch model. This way we can get the correct results. We also have a <em><a class="el" href="retrain_8py.html">retrain.py</a></em> file to train the model even more after its already been saved. The learning rate this way, is a little bit lower than the previous training. Why use Pytorch and not TensorFlow? Pytorch is a more user friendly machine learning framework, and the end result will ultimately be the same between the two. We can also convert the Pytorch model to TensorRT to optimize it for Jetson Nano. Now a little bit on <b>Neural Networks</b>, Neural networks (especially <b>CNNs</b>) excel at learning spatial hierarchies and feature representations directly from pixel data. Random Forest might perform well on clean, structured roads but fail on foggy or nighttime roads where lane markings are faint. CNNs, trained on diverse datasets, adapt much better.</p>
<h1><a class="anchor" id="autotoc_md3"></a>
Hyperparameters</h1>
<p>Change the <b>Batch Size</b> according to your Gpu capabilities, in <em><a class="el" href="dataset_8py.html">dataset.py</a></em>.</p>
<p>We are using the <b>Adam Optimizer</b> for training, as it is well-suited for segmentation tasks due to its adaptive learning rate mechanism. The optimizer is initialized with a standard learning rate of 0.001. However, we are also using a <b>learning rate scheduler</b> to dynamically adjust the learning rate if the loss function shows no improvement over a certain number of epochs. We use a <b>weight decay</b> of 1e-4, this helps preventing overfit.</p>
<p>For the loss function, we combine <b>Focal and Dice Loss</b>. Focal Loss is effective in handling class imbalance by giving more weight to hard-to-classify examples (the incorrect predictions), which is common in segmentation tasks. On the other hand, Dice Loss is great for segmentation because it focuses on the intersection of predicted and ground truth values, rather than the union, making it more sensitive to smaller, harder-to-detect regions.</p>
<p>Additionally, to combat overfitting, we apply random <b>Dropouts</b> in the model's layers during training, which helps prevent the model from depending too much on certain neurons or features, encouraging it to learn more general patterns. We also apply many <b>Transformations</b> to the images, such as Horizontal Flip, Motion and Gaussian blur, and color alterations.</p>
<p>We use <b>Skip Connections</b> to ensure our model doesn't lose important information from earlier layers.</p>
<p>The following is a brief explanation of our model's layers.</p>
<h2><a class="anchor" id="autotoc_md4"></a>
nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1):</h2>
<p>This is a 2D convolutional layer that applies a kernel/filter of size 3x3 across the input image. Padding ensures the output has the same spatial dimensions as the input, keeping the feature map size the same after convolution.</p>
<h2><a class="anchor" id="autotoc_md5"></a>
nn.ReLU(inplace=True):</h2>
<p>Rectified Linear Unit is a non-linear activation function that helps the model learn complex patterns. It outputs the input if it’s positive, or zero if it’s negative. inplace=True means it modifies the input tensor directly, which helps save memory during training.</p>
<h2><a class="anchor" id="autotoc_md6"></a>
nn.GroupNorm(num_groups=32, num_channels=out_channels):</h2>
<p>Normalization is the process of scaling and shifting input data or activations so that they have a more stable distribution, usually with zero mean and unit variance. This helps neural networks learn faster and generalize better.</p>
<h2><a class="anchor" id="autotoc_md7"></a>
nn.MaxPool2d(2):</h2>
<p>Max Pooling reduces the spatial dimensions (height and width) of the input by taking the maximum value from a 2x2 window. It helps reduce the complexity of the model and focuses on the most important features, providing some translation invariance.</p>
<h2><a class="anchor" id="autotoc_md8"></a>
Decoder (ConvTranspose2d):</h2>
<p>This layer is the opposite of a convolution, performing upsampling. This helps increase the spatial dimensions of the feature maps and is used in the decoder part of the network to recreate the original image size.</p>
<h1><a class="anchor" id="autotoc_md9"></a>
Training</h1>
<p>To run this code, you need to install all dependencies such as torch, torchvision and pillow (PIL) To start training the model, insert the right directory path to your training images in <em><a class="el" href="dataset_8py.html">dataset.py</a></em>. We advise you to get access to a good enough gpu and run:</p>
<div class="fragment"><div class="line">python train.py</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md10"></a>
Testing</h1>
<p>To test the model, insert the right directory path to your testing images, in the same script:</p>
<div class="fragment"><div class="line">python testing.py</div>
</div><!-- fragment --><p>For testing in CARLA, be sure to move <a class="el" href="carla__setup_8py.html">carla_setup.py</a> to PythonApi/carla directory in your carla's version folder and run it, after intalling all CARLA's dependencies and running the CARLA's .sh file in another terminal:</p>
<div class="fragment"><div class="line">python carla_setup.py</div>
</div><!-- fragment --><h1><a class="anchor" id="autotoc_md11"></a>
Converting</h1>
<p>To convert the pytorch model to <b>TensorRt</b>, you need to send the trainned model (.pth) to the <b>Jetson Nano</b>, you can do this by running:</p>
<div class="fragment"><div class="line">scp ./pytorch/models/retrain.pth okdot5@10.21.221.43:/home/jetracer/</div>
</div><!-- fragment --><p>After this, run the converting script, be sure to install <b>ONNX</b> as well:</p>
<div class="fragment"><div class="line">python convert.py</div>
</div><!-- fragment --><p>After the conversion, you can apply <b>Pre and Post-processing</b> strategies in C++ so that the model can work correctly. You can see the processing in python here: <b><a class="el" href="carla__setup_8py.html">carla_setup.py</a></b>. </p>
</div></div><!-- contents -->
</div><!-- PageDoc -->
<!-- start footer part -->
<hr class="footer"/><address class="footer"><small>
Generated by&#160;<a href="https://www.doxygen.org/index.html"><img class="footer" src="doxygen.svg" width="104" height="31" alt="doxygen"/></a> 1.9.8
</small></address>
</body>
</html>
